{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questo notebook contiene l'implementazione di BERT (eseguito su google colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "4KyDjHbJXE-v"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SufvA3i6Ylxm",
        "outputId": "69c03d65-f6ee-4d93-cb7d-babc47a78792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch version: 1.13.1+cu116 Device: cuda [Tesla T4]\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    devicename = \"\"\n",
        "    \n",
        "print('Using PyTorch version:', torch.__version__,\n",
        "      'Device:', device, devicename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lbiJDUPYuTm",
        "outputId": "47a190f8-0449-453a-9e05-55151f098af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3150, 5)\n",
            "(3150, 5)\n",
            "(2998, 5)\n",
            "(2196, 5)\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_csv(\"amazon_alexa.tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
        "print(dataset.shape)\n",
        "dataset.dropna(inplace = True)\n",
        "print(dataset.shape)\n",
        "dataset.drop(dataset[dataset.rating == 3].index, inplace=True)\n",
        "print(dataset.shape)\n",
        "dataset.drop_duplicates(subset = \"verified_reviews\", inplace = True)\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi9kcXGsG5lU",
        "outputId": "546348de-d3b4-484f-b762-8b395e221190"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2196, 5)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "DC_htLpo7E7q"
      },
      "outputs": [],
      "source": [
        "X = np.array(dataset[\"verified_reviews\"].values).reshape(-1, 1)\n",
        "y = list(dataset[\"feedback\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_-EYfZN7Bt2",
        "outputId": "c8e52e94-2362-4985-9f80-8d8f021383a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({1: 412, 0: 206})\n"
          ]
        }
      ],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "\n",
        "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state = 0)\n",
        "\n",
        "X, y = undersampler.fit_resample(X, y)\n",
        "\n",
        "\n",
        "print('Resampled dataset shape %s' % Counter(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "U160V_vx9wMT"
      },
      "outputs": [],
      "source": [
        "X_temp = []\n",
        "\n",
        "for rev in X:\n",
        "  X_temp.append(rev[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "t_fYU5HS8rV9"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_temp, y, test_size=0.20, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "8USpyFUA22kL"
      },
      "outputs": [],
      "source": [
        "BERTMODEL = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "Bmgxg4I2Gs4S"
      },
      "outputs": [],
      "source": [
        "ids_train = []\n",
        "ids_test = []\n",
        "amasks_train = []\n",
        "amasks_test = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 512,\n",
        "                        truncation = True,\n",
        "                        padding='max_length',\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "for sample in X_train:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  ids_train.append(encoding_dict['input_ids']) \n",
        "  amasks_train.append(encoding_dict['attention_mask'])\n",
        "\n",
        "ids_train = torch.cat(ids_train, dim = 0)\n",
        "amasks_train = torch.cat(amasks_train, dim = 0)\n",
        "Y_train = torch.tensor(Y_train)\n",
        "\n",
        "for sample in X_test:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  ids_test.append(encoding_dict['input_ids']) \n",
        "  amasks_test.append(encoding_dict['attention_mask'])\n",
        "\n",
        "test_inputs = torch.cat(ids_test, dim = 0)\n",
        "test_masks = torch.cat(amasks_test, dim = 0)\n",
        "test_labels = torch.tensor(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "CKpE8SS-91tg"
      },
      "outputs": [],
      "source": [
        "(train_inputs, validation_inputs,\n",
        " train_labels, validation_labels) = train_test_split(ids_train, Y_train,\n",
        "                                                     random_state=42,\n",
        "                                                     test_size=0.1)\n",
        "(train_masks, validation_masks,\n",
        " _, _) = train_test_split(amasks_train, ids_train,\n",
        "                          random_state=42, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUhFVRsJ38zv",
        "outputId": "632c2989-f2bb-4a7d-a872-46bd556e385a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets:\n",
            "Train: 444 documents\n",
            "Validation: 50 documents\n",
            "Test: 124 documents\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "print('Datasets:')\n",
        "print('Train: ', end=\"\")\n",
        "train_data = TensorDataset(train_inputs, train_masks,\n",
        "                           train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler,\n",
        "                              batch_size=BATCH_SIZE)\n",
        "print(len(train_data), 'documents')\n",
        "\n",
        "print('Validation: ', end=\"\")\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks,\n",
        "                                validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data,\n",
        "                                   sampler=validation_sampler,\n",
        "                                   batch_size=BATCH_SIZE)\n",
        "print(len(validation_data), 'documents')\n",
        "\n",
        "print('Test: ', end=\"\")\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
        "                             batch_size=BATCH_SIZE)\n",
        "print(len(test_data), 'documents')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KxcVhQgATk-",
        "outputId": "259f9ba6-cbd9-4ba0-e90e-b8917fe54d2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained BERT model \"bert-base-uncased\" loaded\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(BERTMODEL,\n",
        "                                                      num_labels=2)\n",
        "model.cuda()\n",
        "print('Pretrained BERT model \"{}\" loaded'.format(BERTMODEL))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxnP4YasAmCL",
        "outputId": "d6ab742c-4e74-449b-9616-dfaf209e6619"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 4\n",
        "WEIGHT_DECAY = 0.01\n",
        "LR = 2e-5\n",
        "WARMUP_STEPS =int(0.2*len(train_dataloader))\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters()\n",
        "                if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': [p for n, p in model.named_parameters()\n",
        "                if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,\n",
        "                                 num_training_steps =len(train_dataloader)*EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "3t8r4bmSA7RD"
      },
      "outputs": [],
      "source": [
        "def train(epoch, loss_vector=None, log_interval=200):\n",
        "      # Set model to training mode\n",
        "  model.train()\n",
        "\n",
        "  # Loop over each batch from the training set\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Copy data to GPU if needed\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Zero gradient buffers\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None,\n",
        "                    attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    if loss_vector is not None:\n",
        "        loss_vector.append(loss.item())\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, step * len(b_input_ids),\n",
        "                len(train_dataloader.dataset),\n",
        "                100. * step / len(train_dataloader), loss))\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "\n",
        "    n_correct, n_all = 0, 0\n",
        "    predicted_labels = []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            logits = outputs[0]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        predictions = np.argmax(logits, axis=1)\n",
        "        predicted_labels.append(predictions)\n",
        "\n",
        "        labels = b_labels.to('cpu').numpy()\n",
        "        n_correct += np.sum(predictions == labels)\n",
        "        n_all += len(labels)\n",
        "\n",
        "    print('Accuracy: [{}/{}] {:.4f}'.format(n_correct, n_all, n_correct/n_all))\n",
        "    return np.concatenate(predicted_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSOww-ojA9b9",
        "outputId": "36c03f93-b4df-4db0-d7fe-9bbe87735b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 1 [0/444 (0%)]\tLoss: 0.664282\n",
            "\n",
            "Validation set:\n",
            "Accuracy: [39/50] 0.7800\n",
            "\n",
            "Train Epoch: 2 [0/444 (0%)]\tLoss: 0.314656\n",
            "\n",
            "Validation set:\n",
            "Accuracy: [45/50] 0.9000\n",
            "\n",
            "Train Epoch: 3 [0/444 (0%)]\tLoss: 0.103013\n",
            "\n",
            "Validation set:\n",
            "Accuracy: [43/50] 0.8600\n",
            "\n",
            "Train Epoch: 4 [0/444 (0%)]\tLoss: 0.071092\n",
            "\n",
            "Validation set:\n",
            "Accuracy: [43/50] 0.8600\n"
          ]
        }
      ],
      "source": [
        "train_lossv = []\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print()\n",
        "    train(epoch, train_lossv)\n",
        "    print('\\nValidation set:')\n",
        "    evaluate(validation_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf8pBnwwBEoD",
        "outputId": "8bc019dd-e94b-4327-f9b9-3859617229f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set:\n",
            "Accuracy: [114/124] 0.9194\n"
          ]
        }
      ],
      "source": [
        "print('Test set:')\n",
        "predictions = evaluate(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV08_RmjBHP1",
        "outputId": "cc3b44e4-715d-4893-f257-3d74ea967e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88        41\n",
            "           1       0.94      0.94      0.94        83\n",
            "\n",
            "    accuracy                           0.92       124\n",
            "   macro avg       0.91      0.91      0.91       124\n",
            "weighted avg       0.92      0.92      0.92       124\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, predictions))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "43973268aa23c7cf4b4cafe0e819fac9b2412fcc991a21988492c3271a4e3f9b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
