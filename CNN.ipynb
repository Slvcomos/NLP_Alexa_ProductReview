{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, MaxPooling1D, Flatten\n",
    "import pandas as pd\n",
    "from preprocessing import *\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.phrases import Phrases\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_25 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"loading pre trained embeddings, this can take some minutes...\")\n",
    "# glove_twitter_25 = KeyedVectors.load_word2vec_format('glove-twitter-25.txt', binary=False)\n",
    "# print(\"loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150, 5)\n",
      "(3150, 5)\n",
      "(2998, 5)\n",
      "(2196, 5)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"amazon_alexa.tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "print(dataset.shape)\n",
    "dataset.dropna(inplace = True)\n",
    "print(dataset.shape)\n",
    "dataset.drop(dataset[dataset.rating == 3].index, inplace=True)\n",
    "print(dataset.shape)\n",
    "dataset.drop_duplicates(subset = \"verified_reviews\", inplace = True)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset[\"verified_reviews\"].values).reshape(-1, 1)\n",
    "y = list(dataset[\"feedback\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({1: 412, 0: 206})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state = 0)\n",
    "\n",
    "X, y = undersampler.fit_resample(X, y)\n",
    "\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = []\n",
    "\n",
    "for rev in X:\n",
    "  X_temp.append(rev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of types extracted is: 1798\n"
     ]
    }
   ],
   "source": [
    "new_text, new_sent_tok = tokenize_list_of_text(X_temp, custom_stopwords, False, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_reviews = list(dataset[dataset[\"feedback\"] == 0][\"verified_reviews\"].values)\n",
    "# print(len(negative_reviews))\n",
    "# negative_text, negative_tok = tokenize_list_of_text(negative_reviews, custom_stopwords, False, pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_artifical = generate_samples(negative_tok, 100, pre_trained_model = glove_twitter_25)\n",
    "# new_sent_tok.extend(negative_artifical)\n",
    "# len(negative_artifical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = frequency_cleaning(new_sent_tok, 2)\n",
    "# negative_artificial_cleaned = cleaned_reviews[-len(negative_artifical):] # estraggo le recensioni artificiali per poi aggiungerle esclusivamente al train\n",
    "# del cleaned_reviews[-len(negative_artifical):] # le elimino dalle recensioni pulite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['little', 'feature']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = Phrases(cleaned_reviews, scoring=\"npmi\", threshold=0.60) #estrae le collocazioni tramite PMI\n",
    "bigrams[cleaned_reviews][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(bigrams[cleaned_reviews], y, test_size=0.20, random_state=10)\n",
    "# X_train.extend(negative_artificial_cleaned)\n",
    "# Y_train.extend([0 for x in range(len(negative_artificial_cleaned))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_train)\n",
    "X_train_encoded = t.texts_to_sequences(X_train)\n",
    "max_length = len(max(bigrams[cleaned_reviews], key = len))\n",
    "Xtrain = pad_sequences(X_train_encoded, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded = t.texts_to_sequences(X_test)\n",
    "Xtest = pad_sequences(X_test_encoded, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({1: 329, 0: 165})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "undersampler_nn = TomekLinks(sampling_strategy=\"all\") # use tomeLinks to remove ambiguous data through nearest neighbours\n",
    "\n",
    "\n",
    "# Fit and transform the X and y data\n",
    "# X_train_resampled, y_train_resampled = undersampler_nn.fit_resample(Xtrain, Y_train)\n",
    "X_train_resampled, y_train_resampled = Xtrain, Y_train\n",
    "\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_train_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train_hot = to_categorical(y_train_resampled)\n",
    "Y_test_hot = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          61800     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 100, 100)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 91, 16)            16016     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 45, 16)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 720)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 720)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               72100     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 190,418\n",
      "Trainable params: 190,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='binary_accuracy', patience=25)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length, trainable = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=16, kernel_size=10, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 - 2s - loss: 0.0162 - binary_accuracy: 0.9873 - val_loss: 2.2864 - val_binary_accuracy: 0.8081 - 2s/epoch - 138ms/step\n",
      "Epoch 2/200\n",
      "13/13 - 0s - loss: 0.0145 - binary_accuracy: 0.9899 - val_loss: 2.4763 - val_binary_accuracy: 0.7980 - 299ms/epoch - 23ms/step\n",
      "Epoch 3/200\n",
      "13/13 - 0s - loss: 0.0374 - binary_accuracy: 0.9848 - val_loss: 2.4035 - val_binary_accuracy: 0.8081 - 286ms/epoch - 22ms/step\n",
      "Epoch 4/200\n",
      "13/13 - 0s - loss: 0.0192 - binary_accuracy: 0.9873 - val_loss: 1.8731 - val_binary_accuracy: 0.8283 - 280ms/epoch - 22ms/step\n",
      "Epoch 5/200\n",
      "13/13 - 0s - loss: 0.0161 - binary_accuracy: 0.9873 - val_loss: 1.8035 - val_binary_accuracy: 0.8333 - 276ms/epoch - 21ms/step\n",
      "Epoch 6/200\n",
      "13/13 - 0s - loss: 0.0157 - binary_accuracy: 0.9861 - val_loss: 1.9120 - val_binary_accuracy: 0.8283 - 277ms/epoch - 21ms/step\n",
      "Epoch 7/200\n",
      "13/13 - 0s - loss: 0.0160 - binary_accuracy: 0.9873 - val_loss: 2.0681 - val_binary_accuracy: 0.8283 - 269ms/epoch - 21ms/step\n",
      "Epoch 8/200\n",
      "13/13 - 0s - loss: 0.0155 - binary_accuracy: 0.9899 - val_loss: 2.1065 - val_binary_accuracy: 0.8283 - 268ms/epoch - 21ms/step\n",
      "Epoch 9/200\n",
      "13/13 - 0s - loss: 0.0168 - binary_accuracy: 0.9873 - val_loss: 2.1704 - val_binary_accuracy: 0.8232 - 291ms/epoch - 22ms/step\n",
      "Epoch 10/200\n",
      "13/13 - 0s - loss: 0.0163 - binary_accuracy: 0.9899 - val_loss: 2.1425 - val_binary_accuracy: 0.8384 - 283ms/epoch - 22ms/step\n",
      "Epoch 11/200\n",
      "13/13 - 0s - loss: 0.0162 - binary_accuracy: 0.9873 - val_loss: 2.1854 - val_binary_accuracy: 0.8182 - 275ms/epoch - 21ms/step\n",
      "Epoch 12/200\n",
      "13/13 - 0s - loss: 0.0157 - binary_accuracy: 0.9873 - val_loss: 2.2061 - val_binary_accuracy: 0.8182 - 275ms/epoch - 21ms/step\n",
      "Epoch 13/200\n",
      "13/13 - 0s - loss: 0.0162 - binary_accuracy: 0.9848 - val_loss: 2.1953 - val_binary_accuracy: 0.8182 - 278ms/epoch - 21ms/step\n",
      "Epoch 14/200\n",
      "13/13 - 0s - loss: 0.0160 - binary_accuracy: 0.9861 - val_loss: 2.2860 - val_binary_accuracy: 0.8182 - 273ms/epoch - 21ms/step\n",
      "Epoch 15/200\n",
      "13/13 - 0s - loss: 0.0158 - binary_accuracy: 0.9899 - val_loss: 2.3753 - val_binary_accuracy: 0.8182 - 275ms/epoch - 21ms/step\n",
      "Epoch 16/200\n",
      "13/13 - 0s - loss: 0.0165 - binary_accuracy: 0.9861 - val_loss: 2.3982 - val_binary_accuracy: 0.8182 - 277ms/epoch - 21ms/step\n",
      "Epoch 17/200\n",
      "13/13 - 0s - loss: 0.0160 - binary_accuracy: 0.9899 - val_loss: 2.4435 - val_binary_accuracy: 0.8131 - 276ms/epoch - 21ms/step\n",
      "Epoch 18/200\n",
      "13/13 - 0s - loss: 0.0159 - binary_accuracy: 0.9899 - val_loss: 2.4830 - val_binary_accuracy: 0.8081 - 278ms/epoch - 21ms/step\n",
      "Epoch 19/200\n",
      "13/13 - 0s - loss: 0.0150 - binary_accuracy: 0.9899 - val_loss: 2.5472 - val_binary_accuracy: 0.8081 - 278ms/epoch - 21ms/step\n",
      "Epoch 20/200\n",
      "13/13 - 0s - loss: 0.0162 - binary_accuracy: 0.9873 - val_loss: 2.6197 - val_binary_accuracy: 0.8081 - 284ms/epoch - 22ms/step\n",
      "Epoch 21/200\n",
      "13/13 - 0s - loss: 0.0158 - binary_accuracy: 0.9873 - val_loss: 2.5231 - val_binary_accuracy: 0.8283 - 280ms/epoch - 22ms/step\n",
      "Epoch 22/200\n",
      "13/13 - 0s - loss: 0.0151 - binary_accuracy: 0.9937 - val_loss: 2.5755 - val_binary_accuracy: 0.8182 - 275ms/epoch - 21ms/step\n",
      "Epoch 23/200\n",
      "13/13 - 0s - loss: 0.0163 - binary_accuracy: 0.9835 - val_loss: 2.6097 - val_binary_accuracy: 0.8182 - 277ms/epoch - 21ms/step\n",
      "Epoch 24/200\n",
      "13/13 - 0s - loss: 0.0178 - binary_accuracy: 0.9899 - val_loss: 2.7051 - val_binary_accuracy: 0.8333 - 279ms/epoch - 21ms/step\n",
      "Epoch 25/200\n",
      "13/13 - 0s - loss: 0.0149 - binary_accuracy: 0.9924 - val_loss: 2.0622 - val_binary_accuracy: 0.8283 - 280ms/epoch - 22ms/step\n",
      "Epoch 26/200\n",
      "13/13 - 0s - loss: 0.0163 - binary_accuracy: 0.9899 - val_loss: 2.0230 - val_binary_accuracy: 0.8283 - 275ms/epoch - 21ms/step\n",
      "Epoch 27/200\n",
      "13/13 - 0s - loss: 0.0164 - binary_accuracy: 0.9886 - val_loss: 2.1845 - val_binary_accuracy: 0.8232 - 281ms/epoch - 22ms/step\n",
      "Epoch 28/200\n",
      "13/13 - 0s - loss: 0.0157 - binary_accuracy: 0.9899 - val_loss: 2.2926 - val_binary_accuracy: 0.8081 - 276ms/epoch - 21ms/step\n",
      "Epoch 29/200\n",
      "13/13 - 0s - loss: 0.0154 - binary_accuracy: 0.9899 - val_loss: 2.3870 - val_binary_accuracy: 0.8081 - 271ms/epoch - 21ms/step\n",
      "Epoch 30/200\n",
      "13/13 - 0s - loss: 0.0160 - binary_accuracy: 0.9899 - val_loss: 2.2916 - val_binary_accuracy: 0.8030 - 274ms/epoch - 21ms/step\n",
      "Epoch 31/200\n",
      "13/13 - 0s - loss: 0.0166 - binary_accuracy: 0.9886 - val_loss: 2.2761 - val_binary_accuracy: 0.8081 - 278ms/epoch - 21ms/step\n",
      "Epoch 32/200\n",
      "13/13 - 0s - loss: 0.0157 - binary_accuracy: 0.9899 - val_loss: 2.3609 - val_binary_accuracy: 0.8081 - 283ms/epoch - 22ms/step\n",
      "Epoch 33/200\n",
      "13/13 - 0s - loss: 0.0166 - binary_accuracy: 0.9899 - val_loss: 2.5878 - val_binary_accuracy: 0.7980 - 285ms/epoch - 22ms/step\n",
      "Epoch 34/200\n",
      "13/13 - 0s - loss: 0.0166 - binary_accuracy: 0.9886 - val_loss: 2.5995 - val_binary_accuracy: 0.8081 - 282ms/epoch - 22ms/step\n",
      "Epoch 35/200\n",
      "13/13 - 0s - loss: 0.0154 - binary_accuracy: 0.9911 - val_loss: 2.5953 - val_binary_accuracy: 0.8081 - 280ms/epoch - 22ms/step\n",
      "Epoch 36/200\n",
      "13/13 - 0s - loss: 0.0155 - binary_accuracy: 0.9899 - val_loss: 2.5970 - val_binary_accuracy: 0.8081 - 340ms/epoch - 26ms/step\n",
      "Epoch 37/200\n",
      "13/13 - 0s - loss: 0.0160 - binary_accuracy: 0.9899 - val_loss: 2.5099 - val_binary_accuracy: 0.8081 - 271ms/epoch - 21ms/step\n",
      "Epoch 38/200\n",
      "13/13 - 0s - loss: 0.0160 - binary_accuracy: 0.9899 - val_loss: 2.4904 - val_binary_accuracy: 0.8030 - 281ms/epoch - 22ms/step\n",
      "Epoch 39/200\n",
      "13/13 - 0s - loss: 0.0145 - binary_accuracy: 0.9899 - val_loss: 2.7572 - val_binary_accuracy: 0.7778 - 292ms/epoch - 22ms/step\n",
      "Epoch 40/200\n",
      "13/13 - 0s - loss: 0.0161 - binary_accuracy: 0.9899 - val_loss: 2.8366 - val_binary_accuracy: 0.7778 - 294ms/epoch - 23ms/step\n",
      "Epoch 41/200\n",
      "13/13 - 0s - loss: 0.0527 - binary_accuracy: 0.9873 - val_loss: 2.3861 - val_binary_accuracy: 0.8030 - 281ms/epoch - 22ms/step\n",
      "Epoch 42/200\n",
      "13/13 - 0s - loss: 0.0266 - binary_accuracy: 0.9848 - val_loss: 1.3150 - val_binary_accuracy: 0.8384 - 275ms/epoch - 21ms/step\n",
      "Epoch 43/200\n",
      "13/13 - 0s - loss: 0.0190 - binary_accuracy: 0.9911 - val_loss: 1.8332 - val_binary_accuracy: 0.7879 - 277ms/epoch - 21ms/step\n",
      "Epoch 44/200\n",
      "13/13 - 0s - loss: 0.0190 - binary_accuracy: 0.9899 - val_loss: 2.2104 - val_binary_accuracy: 0.7677 - 280ms/epoch - 22ms/step\n",
      "Epoch 45/200\n",
      "13/13 - 0s - loss: 0.0160 - binary_accuracy: 0.9911 - val_loss: 2.5913 - val_binary_accuracy: 0.7727 - 276ms/epoch - 21ms/step\n",
      "Epoch 46/200\n",
      "13/13 - 0s - loss: 0.0220 - binary_accuracy: 0.9873 - val_loss: 1.7814 - val_binary_accuracy: 0.7778 - 280ms/epoch - 22ms/step\n",
      "Epoch 47/200\n",
      "13/13 - 0s - loss: 0.0169 - binary_accuracy: 0.9899 - val_loss: 1.7823 - val_binary_accuracy: 0.8333 - 284ms/epoch - 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282c248fb20>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "# fit network\n",
    "model.fit(X_train_resampled, Y_train_hot, epochs=200, verbose=2, validation_split=0.2, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71        41\n",
      "           1       0.86      0.86      0.86        83\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       124\n",
      "   macro avg       0.78      0.78      0.78       124\n",
      "weighted avg       0.81      0.81      0.81       124\n",
      " samples avg       0.81      0.81      0.81       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test_hot, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43973268aa23c7cf4b4cafe0e819fac9b2412fcc991a21988492c3271a4e3f9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
