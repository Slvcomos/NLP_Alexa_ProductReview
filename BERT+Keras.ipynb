{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[tf-cpu]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz5pZs3GViVF",
        "outputId": "143b5bb2-9f6d-43f4-f967-5d329d12e05d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[tf-cpu]\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[tf-cpu]) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[tf-cpu]) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[tf-cpu]) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[tf-cpu]) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[tf-cpu]) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers[tf-cpu]) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers[tf-cpu]) (2.25.1)\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.13.0-py3-none-any.whl (442 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.3/442.3 KB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxconverter-common\n",
            "  Downloading onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-nlp>=0.3.1\n",
            "  Downloading keras_nlp-0.4.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.5/337.5 KB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-cpu<2.12,>=2.4\n",
            "  Downloading tensorflow_cpu-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (221.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[tf-cpu]) (4.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (1.4.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (2.9.2)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.51.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.30.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (57.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (15.0.6.1)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.4.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.19.6)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[tf-cpu]) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[tf-cpu]) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[tf-cpu]) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[tf-cpu]) (1.24.3)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text->transformers[tf-cpu]) (0.12.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.38.4)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.2.2)\n",
            "Installing collected packages: tokenizers, flatbuffers, tensorflow-estimator, onnx, keras, tf2onnx, onnxconverter-common, huggingface-hub, transformers, tensorboard, tensorflow-cpu, tensorflow, tensorflow-text, keras-nlp\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-2.0.7 huggingface-hub-0.12.0 keras-2.11.0 keras-nlp-0.4.0 onnx-1.12.0 onnxconverter-common-1.13.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-cpu-2.11.0 tensorflow-estimator-2.11.0 tensorflow-text-2.11.0 tf2onnx-1.13.0 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow --upgrade-strategy only-if-needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxLBMsI33Uj2",
        "outputId": "f4a0c272-916a-462f-e1a1-58f1211a6419"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.0.7)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRwKOSlnukdB",
        "outputId": "8d68fb6a-e504-4d68-a15f-e17c646f7bd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "Successfully installed scikit-learn-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2kqIE4ncG-h",
        "outputId": "848baa0f-b332-4d2e-cc32-8f74c904942a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Tue_Mar__8_18:18:20_PST_2022\n",
            "Cuda compilation tools, release 11.6, V11.6.124\n",
            "Build cuda_11.6.r11.6/compiler.31057947_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEul-X_ucv3t",
        "outputId": "d9a77258-6126-4dff-8f2d-ffe818387b18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb  9 21:41:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# import all the resources for Natural Language Processing with Python\n",
        "nltk.download(\"all\")\n",
        "import string\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ],
      "metadata": {
        "id": "ZveJ3YLuwePM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fdffb2-63b6-4ce7-d6f0-20d4648c0bc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "# Torch ML libraries\n",
        "import transformers \n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Misc.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "4vq6nIl72-nP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "TklAMacsVfYt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing import *"
      ],
      "metadata": {
        "id": "8QEqS7D-fZ8x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        " \n",
        " \n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "kO8StlSJWL0W"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        " \n",
        "df = pd.read_csv(io.BytesIO(uploaded[\"amazon_alexa.tsv\"]), sep =\"\\t\")\n",
        "df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3SV11oNnWLki",
        "outputId": "632e71e9-5f61-4984-ad43-89b73053cc2c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0  rating       date   variation  \\\n",
              "3145        3145       5  30-Jul-18  Black  Dot   \n",
              "3146        3146       5  30-Jul-18  Black  Dot   \n",
              "3147        3147       5  30-Jul-18  Black  Dot   \n",
              "3148        3148       5  30-Jul-18  White  Dot   \n",
              "3149        3149       4  29-Jul-18  Black  Dot   \n",
              "\n",
              "                                       verified_reviews  feedback  \n",
              "3145  Perfect for kids, adults and everyone in betwe...         1  \n",
              "3146  Listening to music, searching locations, check...         1  \n",
              "3147  I do love these things, i have them running my...         1  \n",
              "3148  Only complaint I have is that the sound qualit...         1  \n",
              "3149                                               Good         1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c322858c-0e71-485c-b0dd-bc3b9d12c222\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>variation</th>\n",
              "      <th>verified_reviews</th>\n",
              "      <th>feedback</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3145</th>\n",
              "      <td>3145</td>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Black  Dot</td>\n",
              "      <td>Perfect for kids, adults and everyone in betwe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3146</th>\n",
              "      <td>3146</td>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Black  Dot</td>\n",
              "      <td>Listening to music, searching locations, check...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3147</th>\n",
              "      <td>3147</td>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Black  Dot</td>\n",
              "      <td>I do love these things, i have them running my...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3148</th>\n",
              "      <td>3148</td>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>White  Dot</td>\n",
              "      <td>Only complaint I have is that the sound qualit...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3149</th>\n",
              "      <td>3149</td>\n",
              "      <td>4</td>\n",
              "      <td>29-Jul-18</td>\n",
              "      <td>Black  Dot</td>\n",
              "      <td>Good</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c322858c-0e71-485c-b0dd-bc3b9d12c222')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c322858c-0e71-485c-b0dd-bc3b9d12c222 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c322858c-0e71-485c-b0dd-bc3b9d12c222');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "df.dropna(inplace = True) #nn ci sono nan ma si fa x sicurezza\n",
        "print(df.shape)\n",
        "df.drop(df[df.rating == 3].index, inplace=True) #commenti neutrali si eliminano\n",
        "print(df.shape)\n",
        "df.drop_duplicates(subset = \"verified_reviews\", inplace = True)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51NrMukmWL-Q",
        "outputId": "dee53a59-5e6d-475d-da5c-0875938ff909"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2196, 6)\n",
            "(2196, 6)\n",
            "(2196, 6)\n",
            "(2196, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Array con reviews che diventano liste a se stanti\n",
        "#Serve x l'undersampling\n",
        "X = np.array(df[\"verified_reviews\"].values).reshape(-1, 1)\n",
        "y = list(df[\"feedback\"].values)"
      ],
      "metadata": {
        "id": "Y_9Uka6wldcG"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "#imbalanced learn - undersampling quando hai classe di minoranza - riduce quella di maggioranza\n",
        "from collections import Counter\n",
        "\n",
        "#sampling_strategy = 0.5 --> classe di minoranza è il 50% della classe di maggioranza\n",
        "\n",
        "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state = 0) \n",
        "\n",
        "X, y = undersampler.fit_resample(X, y)\n",
        "\n",
        "\n",
        "print('Resampled dataset shape %s' % Counter(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cClITYffldm6",
        "outputId": "2239c0df-9598-4312-e664-0198270f1684"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled dataset shape Counter({1: 412, 0: 206})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Riporto allo stato originario - lista di recensioni\n",
        "X_temp = []\n",
        "\n",
        "for rev in X:\n",
        "  X_temp.append(rev[0])"
      ],
      "metadata": {
        "id": "rfQ6YuQNldxE"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text, new_sent_tok = tokenize_list_of_text(X_temp, custom_stopwords, False, 2)\n",
        "# contengono una lista di tutte le frasi pre processate, nella prima variabile in stringa, nella seconda in tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8WFTYdMWMB8",
        "outputId": "dd3d53aa-1017-47b8-8bdc-767c306ed8b9"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of types extracted is: 1788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#divido in 80% -10%\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_text,y, test_size=0.2, random_state=10)\n",
        "#divido in 80%-20%\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.2, random_state=10)\n",
        "\n",
        "print(len(X_train), len(X_val), len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asYG1qaIXaxb",
        "outputId": "b570d03c-daa6-4593-cbb7-6031248ef08b"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "395 99 124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = [len(i.split()) for i in X_train]\n",
        "val = [len(i.split()) for i in X_val]\n",
        "test = [len(i.split()) for i in X_test]\n",
        "\n",
        "plt.hist(train, bins=20, color='b', alpha=0.5, label='train')\n",
        "plt.hist(val, bins=20, color='y', alpha=0.5, label='val')\n",
        "plt.hist(test, bins=20, color='r', alpha=0.5, label='test')\n",
        "plt.xlabel('Lunghezza delle recensioni')\n",
        "plt.ylabel('Numero di recensioni')\n",
        "plt.title('Distribuzione della lunghezza delle recensioni')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "TTBS3TKAWMLq",
        "outputId": "e3f64efe-2c1f-4f7b-a5d7-773f975fa751"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mi calcolo la lunghezza massima delle recensioni\n",
        "def longest_string(list_of_strings):\n",
        "    max_words = 0\n",
        "    for string in list_of_strings:\n",
        "        list_words = string.split()\n",
        "        if len(list_words) > max_words:\n",
        "            max_words = len(list_words)\n",
        "    return max_words\n",
        "\n",
        "max_length = longest_string(new_text)\n",
        "print(max_length)"
      ],
      "metadata": {
        "id": "Tp4hXhUsYzKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c52566-09ae-46e4-ca03-cfaa52e2987f"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "#Tokenizzatore\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "m92-9pZsLZoe"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'output della funzione tokenizer.encode_plus è un dizionario che contiene i seguenti elementi:\n",
        "\n",
        "- input_ids: una lista di interi che rappresenta gli ids tokenizzati delle frasi\n",
        "- attention_mask: una lista di 0 e 1 che indica se un determinato token è reale o solo un padding\n",
        "- token_type_ids (opzionale): una lista di 0 e 1 che indica se un token appartiene alla prima o alla seconda metà di una frase originale divisa in due frasi tokenizzate\n",
        "Per ottenere tensori con le stesse dimensioni, è necessario utilizzare lo stesso numero di frasi e la stessa lunghezza massima per ogni frase nella chiamata a tokenizer.encode_plus.\n",
        "\n",
        "La forma dei tensori input_ids e attention_masks dovrebbe essere (len(encoded_dict['input_ids']), max_length) e (len(encoded_dict['attention_mask']), max_length) rispettivamente perché ogni esempio di input è una matrice con lunghezza max_length, non un vettore con lunghezza 1"
      ],
      "metadata": {
        "id": "X1-_FZjpPqXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(sentences):\n",
        "  #mi creo le due liste che conterranno gli ids e le attention mask  \n",
        "  input_ids_list = []\n",
        "  attention_masks_list = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(sentence, max_length=max_length,add_special_tokens = True, pad_to_max_length=True, return_attention_mask=True)\n",
        "    \n",
        "    #converto in tensori ogni ids e attention mask di ogni recensione e li appendo alle liste\n",
        "    input_ids = tf.convert_to_tensor(encoded_dict['input_ids'])\n",
        "    attention_masks = tf.convert_to_tensor(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids_list.append(input_ids)\n",
        "    attention_masks_list.append(attention_masks)\n",
        "\n",
        "  #La funzione stack serve a concatenare i tensori contenuti in input_ids_list e attention_masks_list lungo un nuovo asse, \n",
        "  #creando così un tensore più grande che rappresenta tutti i tensori singoli concatenati.\n",
        "  #Senza avrei una lista di tensori singoli, mentre il modello richiede tensori di forma uniforme come input.\n",
        "  input_ids = tf.stack(input_ids_list)\n",
        "  attention_masks = tf.stack(attention_masks_list)\n",
        "\n",
        "  return input_ids, attention_masks\n",
        "\n"
      ],
      "metadata": {
        "id": "8szPXW1mY2OF"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_masks = prepare_data(X_train)\n",
        "val_input_ids, val_masks = prepare_data(X_val)\n",
        "test_input_ids, test_masks = prepare_data(X_test)"
      ],
      "metadata": {
        "id": "9m32HmTcY5CE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47b0722-42ab-4cea-84e5-489e5d668ba7"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trasformo in tensori anche le etichette, altrimenti il modello nell'addestramento mi da errore\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "LZxNrCE1uMq2"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_input_ids.shape)\n",
        "print(train_masks.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EiWyPoEZ9KU",
        "outputId": "8e941c13-109e-4148-819d-64455e8a2645"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(395, 134)\n",
            "(395, 134)\n",
            "(395,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', X_train[0])\n",
        "print('Token IDs:', train_input_ids[0])\n",
        "print('Attention Mask:', train_masks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSyJgDuy7OGl",
        "outputId": "acdc2272-c7f2-45ce-bae8-3e6adb3b25f3"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  love\n",
            "Token IDs: tf.Tensor(\n",
            "[ 101 1567  102    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0], shape=(134,), dtype=int32)\n",
            "Attention Mask: tf.Tensor(\n",
            "[1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(134,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(train_input_ids.shape) > 1:\n",
        "    data= train_input_ids[0]\n",
        "\n",
        "print(tokenizer.decode(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_saXPbw37lJN",
        "outputId": "2813a4be-0ddb-46ae-ba5e-f765f5447a81"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] love [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "input_ids è un tensore di forma (numero di frasi, lunghezza massima), che rappresenta gli ID numerici degli token delle frasi in input.\n",
        "\n",
        "attention_masks è un tensore di forma (numero di frasi, lunghezza massima), che rappresenta una maschera per indicare quali elementi del tensore input_ids devono essere considerati durante il modello di elaborazione del linguaggio.\n"
      ],
      "metadata": {
        "id": "5uQKcPoKYKsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n"
      ],
      "metadata": {
        "id": "TBeNaEETR7oH"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout**\n",
        "A dropout layer randomly hides output of units from a layer\n",
        "to the next.\n",
        "● It is a regularization technique that contrasts overfitting\n",
        "(i.e., being too accurate on training data and not learning\n",
        "to generalize).\n",
        "● It can also help breaking cases of symmetry in the\n",
        "network.\n",
        "\n",
        "Il dropout è una tecnica di regolarizzazione utilizzata in deep learning per prevenire overfitting. L'overfitting si verifica quando il modello ha un'alta precisione sui dati di addestramento ma bassa precisione sui dati di test.\n",
        "\n",
        "Il dropout funziona eliminando casualmente alcuni nodi di un layer durante l'addestramento, impedendo così al modello di essere troppo aderente ai dati di addestramento. Questo aumenta la generalizzazione del modello e migliora le sue prestazioni sui dati di test.\n",
        "\n",
        "In pratica, il dropout viene applicato inserendo un layer di dropout tra i layer del modello. Durante l'addestramento, alcuni nodi vengono disattivati con una certa probabilità (detto tasso di dropout), e durante la valutazione vengono attivati tutti i nodi.\n",
        "\n",
        "Both **Sequential** and Model classes can be used to define a Keras model, and the choice between them depends on the specific requirements of the model you're building.\n",
        "\n",
        "Sequential is a linear stack of layers, where you use the large list of available layers in Keras, such as Dense, Conv2D, LSTM, etc., and you use them in a linear stack of layers. It's simpler and easier to use, especially for small or simple models.\n",
        "\n",
        "**Model** is more flexible and allows you to create models that are more complex, with multiple inputs, outputs, or shared layers."
      ],
      "metadata": {
        "id": "r11KfUZnxuga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    # BERT encoder\n",
        "    encoder = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "    \n",
        "    # # Unfreeze the BERT layers for fine-tuning\n",
        "    # encoder.trainable = False\n",
        "\n",
        "    input_ids = layers.Input(shape=(max_length,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_length,), dtype=tf.int32)\n",
        "    \n",
        "    #il modello BERT restituisce 2 oggetti, il primo che è la rappresentazione vettoriale del testo di input,\n",
        "    #il secondo che rappresenta il risultato della pooling sull aprima colonna della matrice di attenzione\n",
        "    embedding = encoder(\n",
        "        input_ids, attention_mask=attention_mask\n",
        "    )[0]\n",
        "\n",
        "    dropout = layers.Dropout(0.2)(embedding)\n",
        "\n",
        "    dense_layer = Dense(128, activation='relu')(dropout)\n",
        "    \n",
        "    #Il tasso di dropout del 20% è un valore di default comunemente utilizzato,\n",
        "    # ma può essere regolato per ottenere il miglior equilibrio tra overfitting e generalizzazione del modello. \n",
        "    #Il tasso di dropout specifica la quantità di neuroni che verranno casualmente disattivati durante la formazione del modello\n",
        "    dropout = layers.Dropout(0.2)(dense_layer)\n",
        "    #I dati di input vengono trasformati in embedding  e passati a un layer \"dense\" con funzione di attivazione sigmoid\n",
        "    class_prob = layers.Dense(1, activation='sigmoid')(dropout)\n",
        "    class_prob = layers.Flatten()(class_prob)\n",
        "\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, attention_mask],\n",
        "        outputs=class_prob,\n",
        "    )\n",
        "\n",
        "    #la funzione di perdita - binary cross-entropy\n",
        "    loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    #ottimizzatore Adam\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=2e-5)\n",
        "    \n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "bZ88N8f8Kb_Y"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJZ4exR4K8-g",
        "outputId": "97b82cfb-e1ac-4ded-999c-b7f3b3b42ab0"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_35 (InputLayer)          [(None, 134)]        0           []                               \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 134)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_for_sequence_classific  TFSequenceClassifie  108311810  ['input_35[0][0]',               \n",
            " ation_17 (TFBertForSequenceCla  rOutput(loss=None,               'input_36[0][0]']               \n",
            " ssification)                   logits=(None, 2),                                                 \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " dropout_718 (Dropout)          (None, 2)            0           ['tf_bert_for_sequence_classifica\n",
            "                                                                 tion_17[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 128)          384         ['dropout_718[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_719 (Dropout)          (None, 128)          0           ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 1)            129         ['dropout_719[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_17 (Flatten)           (None, 1)            0           ['dense_35[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,312,323\n",
            "Trainable params: 108,312,323\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Addestramento del modello\n",
        "training = model.fit([train_input_ids, train_masks], y_train, epochs=8,batch_size=16, validation_data=([val_input_ids, val_masks], y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT-iYJRhf5sQ",
        "outputId": "948cf7b6-af77-4f92-9c72-0a4c3d4af807"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "25/25 [==============================] - 12s 484ms/step - loss: 0.5133 - accuracy: 0.7494 - val_loss: 0.4821 - val_accuracy: 0.8586\n",
            "Epoch 2/8\n",
            "25/25 [==============================] - 12s 474ms/step - loss: 0.4072 - accuracy: 0.8152 - val_loss: 0.4418 - val_accuracy: 0.8485\n",
            "Epoch 3/8\n",
            "25/25 [==============================] - 12s 470ms/step - loss: 0.3430 - accuracy: 0.8481 - val_loss: 0.4011 - val_accuracy: 0.8788\n",
            "Epoch 4/8\n",
            "25/25 [==============================] - 12s 474ms/step - loss: 0.3312 - accuracy: 0.8886 - val_loss: 0.3990 - val_accuracy: 0.8687\n",
            "Epoch 5/8\n",
            "25/25 [==============================] - 12s 475ms/step - loss: 0.2949 - accuracy: 0.9114 - val_loss: 0.3997 - val_accuracy: 0.8485\n",
            "Epoch 6/8\n",
            "25/25 [==============================] - 12s 476ms/step - loss: 0.2603 - accuracy: 0.9013 - val_loss: 0.3628 - val_accuracy: 0.8889\n",
            "Epoch 7/8\n",
            "25/25 [==============================] - 12s 466ms/step - loss: 0.2429 - accuracy: 0.8987 - val_loss: 0.3816 - val_accuracy: 0.8889\n",
            "Epoch 8/8\n",
            "25/25 [==============================] - 12s 470ms/step - loss: 0.2141 - accuracy: 0.9063 - val_loss: 0.4025 - val_accuracy: 0.8687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'overfitting si verifica quando il modello ha una bassa perdita (loss) sul set di allenamento, ma una più alta perdita sulla valutazione. In questo caso, i valori di perdita e precisione (accuracy) sul set di valutazione sono relativamente costanti durante i diversi epoch, il che suggerisce che il modello non sta overfitting.    \n",
        "    \"loss\" indica la funzione di perdita, ovvero la misura della differenza tra le previsioni del modello e i valori effettivi. Una perdita più bassa indica che il modello sta facendo previsioni più accurate.\n",
        "    \"accuracy\" è la precisione, ovvero la percentuale di previsioni corrette effettuate dal modello.\n",
        "    \"val_loss\" e \"val_accuracy\" sono rispettivamente la perdita e la precisione calcolate sul set di dati di convalida, che vengono utilizzati per valutare la generalizzabilità del modello."
      ],
      "metadata": {
        "id": "GOtYnqJEtpCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valutazione del modello\n",
        "results = model.evaluate([val_input_ids, val_masks], y_val)"
      ],
      "metadata": {
        "id": "eL8mYNAEfgI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0230aed-7eee-43f9-d735-4f23fa48c819"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 206ms/step - loss: 0.4025 - accuracy: 0.8687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "risultati = model.evaluate([test_input_ids, test_masks], y_test)\n",
        "loss, accuracy = risultati[0], risultati[1]\n",
        "print(\"Test Loss: {:.4f} Test Accuracy: {:.4f}\".format(loss, accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsir-MlPK9Be",
        "outputId": "9b7d8ba2-440e-4243-ad27-2fceed75d927"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 277ms/step - loss: 0.3787 - accuracy: 0.8710\n",
            "Test Loss: 0.3787 Test Accuracy: 0.8710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eseguire le previsioni sul set di test\n",
        "predictions = model.predict([test_input_ids, test_masks])\n",
        "\n",
        "# Convertire le previsioni in classi binarie (0 o 1) utilizzando una soglia di 0,5\n",
        "predictions_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Valutare le prestazioni del modello\n",
        "accuracy = accuracy_score(y_test, predictions_classes)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "NaVgjz5fK9Eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af87d665-79c6-4362-acc4-24df45e12c0b"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 275ms/step\n",
            "Accuracy:  0.8709677419354839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.round(predictions)"
      ],
      "metadata": {
        "id": "j6rJ08zx08GY"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YjorXuP8cUD",
        "outputId": "8d0b38e7-a16a-40be-e91d-5a8cf1aefdac"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.93      0.87        41\n",
            "         1.0       0.96      0.90      0.93        83\n",
            "\n",
            "    accuracy                           0.91       124\n",
            "   macro avg       0.89      0.92      0.90       124\n",
            "weighted avg       0.92      0.91      0.91       124\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kkJ0DE_mAaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqX70d3dmAc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNunjRfbmAfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2SJO9uNNmAir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEb9k3r4qdGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMJK2SBtqjpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oHcmw_O2qjsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0saq_2TYqjvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bmEVYimdfqLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}